---
title: "CSS Web Scraping and Final Case Study"
output: html_document
---

```{r, include=FALSE}
library(httr)
library(rvest)
library(xml2)
```

# CSS Web Scraping and Final Case Study

CSS path-based web scraping is a far-more-pleasant alternative to using XPATHs. You'll start this chapter by learning about CSS, and how to leverage it for web scraping. Then, you'll work through a final case study that combines everything you've learnt so far to write a function that queries an API, parses the response and returns data in a nice form.

## CSS web scraping in theory

__CSS__

```
.class_a {
  color: black; }
.class_b {
  color: red; 
}
```

__HTML__

```
<a class = "class_a" href = "http://en.wikipedia.org/"> This is black </a> 
<a class = "class_b" href = "http://en.wikipedia.org/"> This is red </a>
```

__CSS versus XPATH__

- CSS scraping looks for class or id names (e.g. "class_a")
- Much like XPATH
- ...but grabs multiple things
- Still use html_node() (or html_nodes()), but with css argument

## Using CSS to scrape nodes {.tabset .tabset-fade .tabset-pills}

### Exercise

As mentioned in the video, CSS is a way to add design information to HTML, that instructs the browser on how to display the content. You can leverage these design instructions to identify content on the page.

You've already used `html_node()`, but it's more common with CSS selectors to use `html_nodes()` since you'll often want more than one node returned. Both functions allow you to specify a `css` argument to use a CSS selector, instead of specifying the `xpath` argument.

What do CSS selectors look like? Try these examples to see a few possibilities.

### Instructions

We've read in the same HTML page from Chapter 4, the Wikipedia page for Hadley Wickham, into `test_xml`.

- Use the CSS selector `"table"` to select all elements that are a `table` tag.
- Use the CSS selector `".infobox"` to select all elements that have the attribute `class = "infobox"`.
- Use the CSS selector `"#firstHeading"` to select all elements that have the attribute `id = "firstHeading"`.

### script.R

```{r, collapse=TRUE}
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

test_xml <- read_html(test_url)

# Select the table elements
html_nodes(test_xml, css = "table")

# Select elements with class = "infobox"
html_nodes(test_xml, css = ".infobox")

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = "#firstHeading")
```

## Scraping names {.tabset .tabset-fade .tabset-pills}

### Exercise

You might have noticed in the previous exercise, to select elements with a certain class, you add a `.` in front of the class name. If you need to select an element based on its id, you add a `#` in front of the id name.

For example if this element was inside your HTML document:

```
<h1 class = "heading" id = "intro">
  Introduction
</h1>
```

You could select it by its class using the CSS selector `".heading"`, or by its id using the CSS selector `"#intro"`.

Once you've selected an element with a CSS selector, you can get the element tag name just like you did with XPATH selectors, with `html_name()`. Try it!

### Instructions

- The infobox you extracted in Chapter 4 has the class infobox. Use `html_nodes()` and the appropriate CSS selector to extract the infobox element to `infobox_element`.
- Use `html_name()` to extract the tag name of `infobox_element` and store it in `element_name`.
- Print `element_name`.

### script.R

```{r, collapse=TRUE}
# Extract element with class infobox
infobox_element <- html_nodes(test_xml, css = ".infobox")

# Get tag name of infobox_element
element_name <- html_name(infobox_element)

# Print element_name
element_name
```

## Scraping text {.tabset .tabset-fade .tabset-pills}

### Exercise

Of course you can get the contents of a node extracted using a CSS selector too, with `html_text()`.

Can you put the pieces together to get the page title like you did in Chapter 4?

### Instructions

The infobox HTML element is stored in infobox_element in your workspace.

Use `html_node()` to extract the element from `infobox_element` with the CSS class `fn`.
Use `html_text()` to extract the contents of `page_name`.
Print `page_title`.

### script.R

```{r, collapse=TRUE}
# Extract element with class fn
page_name <- html_node(x = infobox_element, ".fn")

# Get contents of page_name
page_title <- html_text(page_name)

# Print page_title
page_title
```

## Test: CSS web scraping {.tabset .tabset-fade .tabset-pills}

### question

Take a look at the chunk of HTML being read into `test`:

``` {r, eval=FALSE}
test <- read_html('
   <h1 class = "main">Hello world!</h1>
   ')
```

How would you extract the text `Hello world!` using `rvest` and CSS selectors?

### Answer

Possible Answers

[x] html_name(html_node(test, css = ".main"))
[x] html_name(html_node(test, css = "#main"))
[x] html_text(html_node(test, css = "#main"))
[o] html_text(html_node(test, css = ".main"))

```{r, collapse=TRUE}
library(rvest)

test <- read_html('<h1 class = "main">Hello world!</h1>')
html_text(html_node(test, css = ".main"))
```

## Final case study: Introduction

__What we'll cover__

Extracting an infobox from a Wikipedia page

1. Get the XML content of a Wikipedia page through API 
2. Extract the infobox from the page
3. Clean it up and turn it into a data frame
4. Turn it into a function

## API calls {.tabset .tabset-fade .tabset-pills}

### Exercise

Your first step is to use the Wikipedia API to get the page contents for a specific page. We'll continue to work with the Hadley Wickham page, but as your last exercise, you'll make it more general.

To get the content of a page from the Wikipedia API you need to use a parameter based URL. The URL you want is

`https://en.wikipedia.org/w/api.php?action=parse&page=Hadley%20Wickham&format=xml`

which specifies that you want the parsed content (i.e the HTML) for the "Hadley Wickham" page, and the API response should be XML.

In this exercise you'll make the request with `GET()` and parse the XML response with `content()`.

### Instructions

We've already defined `base_url` for you.

- Create a list for the query parameters, setting `action = "parse"`, `page = "Hadley Wickham"` and `format = "xml"`.
- Use `GET()` to call the API by specifying url and query.
- Parse the response using `content()`.

### script.R

```{r, collapse=TRUE}
# Load httr
library(httr)

# The API url
base_url <- "https://en.wikipedia.org/w/api.php"

# Set query parameters
query_params <- list(action = "parse", 
  page = "Hadley Wickham", 
  format = "xml")

# Get data from API
resp <- GET(url = base_url, query = query_params)
    
# Parse response
resp_xml <- content(resp)
```

## Extracting information {.tabset .tabset-fade .tabset-pills}

### Exercise

Now we have a response from the API, we need to extract the HTML for the page from it. It turns out the HTML is stored in the contents of the XML response.
Take a look, by using `xml_text()` to pull out the text from the XML response:

`xml_text(resp_xml)`

In this exercise, you'll read this text as HTML, then extract the relevant nodes to get the infobox and page title.

### Instructions

Code from the previous exercise has already been run, so you have `resp_xml` available in your workspace.

- Use `read_html()` to read the contents of the XML response (`xml_text(resp_xml)`) as HTML.
- Use `html_node()` to extract the `infobox` element (having the class infobox) from `page_html` with a CSS selector.
- Use `html_node()` to extract the page title element (having the class fn) from `infobox_element` with a CSS selector.
- Extract the title text from `page_name` with `html_text()`.

### script.R

```{r, collapse=TRUE}
# Load rvest
library(rvest)

# Read page contents as HTML
page_html <- read_html(xml_text(resp_xml))

# Extract infobox element
infobox_element <- html_node(page_html, css = ".infobox")

# Extract page name element from infobox
page_name <- html_node(infobox_element, css = ".fn")

# Extract page name as text
page_title <- html_text(page_name)
```

## Normalising information {.tabset .tabset-fade .tabset-pills}

### Exercise

Now it's time to put together the information in a nice format. You've already seen you can use `html_table()` to parse the infobox into a data frame. But one piece of important information is missing from that table: who the information is about!

In this exercise, you'll parse the infobox in a data frame, and add a row for the full name of the subject.

### Instructions

No need to repeat all the table parsing code from Chapter 4, we've already added it to your script.

- Create a new data frame where `key` is the string `"Full name"` and `value` is our previously stored `page_title`.
- Combine `name_df` with `cleaned_table` using `rbind()` and assign it to `wiki_table2`.
- Print `wiki_table2`.

### script.R

```{r, collapse=TRUE}
# Your code from earlier exercises
wiki_table <- html_table(infobox_element)
colnames(wiki_table) <- c("key", "value")
cleaned_table <- subset(wiki_table, !key == "")

# Create a dataframe for full name
name_df <- data.frame(key = "Full name", value = page_title)

# Combine name_df with cleaned_table
wiki_table2 <- rbind(name_df, cleaned_table)

# Print wiki_table
wiki_table2
```

## Reproducibility {.tabset .tabset-fade .tabset-pills}

### Exercise

Now you've figured out the process for requesting and parsing the infobox for the Hadley Wickham page, it's time to turn it into a function that does the same thing for anyone.

You've already done all the hard work! In the sample script we've just copied all your code from the previous three exercises, with only one change: we've wrapped it in the function definition syntax, and chosen the name `get_infobox()` for this function.

It doesn't quite work yet, the argument `title` isn't used inside the function. In this exercise you'll fix that, then test it out with some other personalities.

### Instructions

- Fix the function, by replacing the string `"Hadley Wickham"` with `title`, so that the title argument of the function will be used for the query.
- Test `get_infobox()` with `title = "Hadley Wickham"`.
- Now, try getting the infobox for `"Ross Ihaka"`.
- Finally, try getting the infobox for `"Grace Hopper"`.

### script.R

```{r, collapse=TRUE}
library(httr)
library(rvest)
library(xml2)

get_infobox <- function(title){
  base_url <- "https://en.wikipedia.org/w/api.php"
  
  # Change "Hadley Wickham" to title
  query_params <- list(action = "parse", 
    page = title, 
    format = "xml")
  
  resp <- GET(url = base_url, query = query_params)
  resp_xml <- content(resp)
  
  page_html <- read_html(xml_text(resp_xml))
  infobox_element <- html_node(x = page_html, css =".infobox")
  page_name <- html_node(x = infobox_element, css = ".fn")
  page_title <- html_text(page_name)
  
  wiki_table <- html_table(infobox_element)
  colnames(wiki_table) <- c("key", "value")
  cleaned_table <- subset(wiki_table, !wiki_table$key == "")
  name_df <- data.frame(key = "Full name", value = page_title)
  wiki_table <- rbind(name_df, cleaned_table)
  
  wiki_table
}

# Test get_infobox with "Hadley Wickham"
get_infobox(title = "Hadley Wickham")

# Try get_infobox with "Ross Ihaka"
get_infobox(title = "Ross Ihaka")

# Try get_infobox with "Grace Hopper"
get_infobox(title = "Grace Hopper")
```

## Wrap Up

- Downloading and reading flat files 
- Designing and using API clients 
- Web scraping: XPATHs & CSS