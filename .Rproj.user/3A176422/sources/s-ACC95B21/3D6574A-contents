---
title: "Downloading Files and Using API Clients"
output: html_document
---

# Downloading Files and Using API Clients

Sometimes getting data off the internet is very, very simple - it's stored in a format that R can handle and just lives on a server somewhere, or it's in a more complex format and perhaps part of an API but there's an R package designed to make using it a piece of cake. This chapter will explore how to download and read in static files, and how to use APIs when pre-existing clients are available.


## Introduction: Working With Web Data in R

- Downloading files and using specialised packages to get data from web
- httr package to query APIs using GET() and POST()
- JSON and XML: data formats commonly returned
- CSS to navigate and extract data from webpages

## Downloading files and reading them into R {.tabset .tabset-fade .tabset-pills}

### Exercise

In this first exercise we're going to look at reading already-formatted datasets - CSV or TSV files, with which you'll no doubt be familiar! - into R from the internet. This is a lot easier than it might sound because R's file-reading functions accept not just file paths, but also URLs.

### Instructions

The URLs to those files are in your R session as csv_url and tsv_url.

- Read the CSV file stored at csv_url into R using read.csv(). Assign the result to csv_data.
- Do the same for the TSV file stored at tsv_url using read.delim(). Assign the result to tsv_data.
- Examine each object using head().

### script.R

```{r, collapse=TRUE}
# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)
```


## Saving raw files to disk  {.tabset .tabset-fade .tabset-pills}

### Exercise

Sometimes just reading the file in from the web is enough, but often you'll want to store it locally so that you can refer back to it. This also lets you avoid having to spend the start of every analysis session twiddling your thumbs while particularly large files download.

Helpfully, R has download.file(), a function that lets you do just that: download a file to a location of your choice on your computer. It takes two arguments; `url`, indicating the URL to read from, and `destfile`, the destination to write the downloaded file to. In this case, we've pre-defined the URL - once again, it's `csv_url`.

### Instructions

Download the file at `csv_url` with `download.file()`, naming the destination file `"feed_data.csv"`.
Read `"feed_data.csv"` into R with read.csv().

### script.R

```{r, collapse=TRUE}
# Download the file with download.file()
download.file(url = csv_url, destfile = "feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
```

## Saving formatted files to disk  {.tabset .tabset-fade .tabset-pills}

### Exercise

Whether you're downloading the raw files with download.file() or using read.csv() and its sibling functions, at some point you're probably going to find the need to modify your input data, and then save the modified data to disk so you don't lose the changes.

You could use write.table(), but then you have to worry about accidentally writing out data in a format R can't read back in. An easy way to avoid this risk is to use saveRDS() and readRDS(), which save R objects in an R-specific file format, with the data structure intact. That means you can use it for any type of R object (even ones that don't turn into tables easily), and not worry you'll lose data reading it back in. saveRDS() takes two arguments, object, pointing to the R object to save and file pointing to where to save it to. readRDS() expects file, referring to the path to the RDS file to read in.

In this example we're going to modify the data you already read in, which is predefined as csv_data, and write the modified version out to a file before reading it in again.

### Instructions

- Modify csv_data to add the column square_weight, containing the square of the weight column.
- Save it to disk as "modified_feed_data.RDS" with saveRDS().
- Read it back in as modified_feed_data with readRDS().
- Examine modified_feed_data.

### script.R

```{r, collapse=TRUE}
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight ^ 2

# Save it to disk with saveRDS()
saveRDS(object = csv_data, file = "modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS(file = "modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)
```

## Understanding Application Programming Interfaces

- 'websites, but for machines'
- Can be used to expose data automatically
- Lets you make queries for specific bits of that data

API Clients

- Native (in R!) interfaces to APIs Hides API complexity
- Lets you read data in as R objects

Using API Clients

- Always use a client if you can
- Find them by googling 'CRAN [name of website]' Only write code you have to write

pageviews

```{r, eval=FALSE}
library(pageviews)
article_pageviews(article = project = "en.wikipedia", "R_(programming_language)")
```

## API test {.tabset .tabset-fade .tabset-pills}

### Question

In the last video you were introduced to Application Programming Interfaces, or APIs, along with their intended purpose (as the computer equivalent of the visible web page that you and I might interact with) and their utility for data retrieval. What are APIs for?

### Answer 

Possible Answers

[x] Making parts of a website available to people.

[x] Making parts of a website available to puppies.

[o] Making parts of a website available to computers.

## Using API clients {.tabset .tabset-fade .tabset-pills}

### Exercise

So we know that APIs are server components to make it easy for your code to interact with a service and get data from it. We also know that R features many "clients" - packages that wrap around connections to APIs so you don't have to worry about the details.

Let's look at a really simple API client - the pageviews package, which acts as a client to Wikipedia's API of pageview data. As with other R API clients, it's formatted as a package, and lives on CRAN - the central repository of R packages. The goal here is just to show how simple clients are to use: they look just like other R code, because they are just like other R code.

### Instructions

- Load the package pageviews.
- Use the article_pageviews() function to get the pageviews for the article "Hadley Wickham".
- Examine the resulting object.

### script.R

```{r, collapse=TRUE}
# Load pageviews
library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
```

## Access tokens and APIs

API etiquette

- Overwhelming the API means you can't use it 
- Overwhelming the API means nobody else can use it APIs 
- issue 'access tokens' to control and identify use

Getting access tokens

- Usually requires registering your email address 
- Sometimes providing an explanation
- Example: https://www.wordnik.com/ which requires both!

birdnik

- birdnik a package that wraps the Wordnik API 
- Provide API key in key argument in birdnik functions

## Using access tokens {.tabset .tabset-fade .tabset-pills}

### Exercise

As we discussed in the last video, it's common for APIs to require access tokens - unique keys that verify you're authorised to use a service. They're usually pretty easy to use with an API client.

To show how they work, and how easy it can be, we're going to use the R client for the Wordnik dictionary and word use service - 'birdnik' - and an API token we prepared earlier. Birdnik is fairly simple (I wrote it!) and lets you get all sorts of interesting information about word usage in published works. For example, to get the frequency of the use of the word "chocolate", you would write:

> word_frequency(api_key, "chocolate")

In this exercise we're going to look at the word "vector" (since it's a common word in R!) using a pre-existing API key (stored as `api_key`)

### Instructions

- Load the package `birdnik`.
- Using the pre-existing API key and [word_frequency()](https://www.rdocumentation.org/packages/birdnik/topics/word_frequency), get the frequency of the word `"vector"` in Wordnik's database. Assign the results to `vector_frequency`.

### script.R

```{r, collapse=TRUE}
if (!require('birdnik'))devtools::install_github("ironholds/birdnik")

# Load birdnik
library(birdnik)

# Get the word frequency for "vector", using api_key to access it

api_key <- "d8ed66f01da01b0c6a0070d7c1503801993a39c126fbc3382"
vector_frequency <- word_frequency(api_key, "vector")
```



-------



# Using httr to interact with APIs directly

If an API client doesn't exist, it's up to you to communicate directly with the API. But don't worry, the package `httr` makes this really straightforward. In this chapter you'll learn how to make web requests from R, how to examine the responses you get back and some best practices for doing this in a responsible way.

## GET and POST requests in theory

HTTP requests

- Conversation between your machine and the server 
- First: what you want to happen
- "methods" - different requests for different tasks

GET and POST

- GET: 'get me something'
- POST: 'have something of mine'

Other types

- HEAD - just like head()
- DELETE - 'remove this thing'
- Many others! But GET and POST are the big ones

Making GET requests with httr

> response <- GET(url = "https://httpbin.org/get") > content(response)
$args
named list()
$headers
$headers$Accept
[1] "application/json, text/xml, application/xml, */*" ...

Making POST requests with httr
> response <- POST(url = "https://httpbin.org/post")

## GET requests in practice {.tabset .tabset-fade .tabset-pills}

### Exercise

To start with you're going to make a GET request. As discussed in the video, this is a request that asks the server to give you a particular piece of data or content (usually specified in the URL). These make up the majority of the requests you'll make in a data science context, since most of the time you'll be getting data from servers, not giving it to them.

To do this you'll use the `httr` package, written by Hadley Wickham (of course), which makes HTTP requests extremely easy. You're going to make a very simple GET request, and then inspect the output to see what it looks like.

### Instructions

- Load the httr package.
- Use the GET() function to make a request to http://httpbin.org/get, saving the result to `get_result`.
- Print `get_result` to inspect it.

### script.R

```{r, collapse=TRUE}
# Load the httr package
library("httr")

# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")

# Print it to inspect it
get_result
```

## POST requests in practice {.tabset .tabset-fade .tabset-pills}

### Exercies

Next we'll look at POST requests, also made through httr, with the function (you've guessed it) POST(). Rather than asking the server to give you something, as in GET requests, a POST request asks the server to accept something from you. They're commonly used for things like file upload, or authentication. As a result of their use for uploading things, `POST()` accepts not just a `url` but also a `body` argument containing whatever you want to give to the server.

You'll make a very simple POST request, just uploading a piece of text, and then inspect the output to see what it looks like.

### Instructions

- Load the httr package.
- Make a POST request with the URL http://httpbin.org/post and the body `"this is a test"`, saving the result to post_result.
- Print post_result to inspect it.

### script.R

```{r, collapse=TRUE}
# Load the httr package
library(httr)

# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST(url = "http://httpbin.org/post", body = "this is a test")

# Print it to inspect it
post_result
```

## Extracting the response {.tabset .tabset-fade .tabset-pills}

### Exercies

Making requests is all well and good, but it's also not why you're here. What we really want to do is get the data the server sent back, which can be done with httr's `content()` function. You pass it an object returned from a `GET` (or `POST`, or `DELETE`, or...) call, and it spits out whatever the server actually sent in an R-compatible structure.

We're going to demonstrate that now, using a slightly more complicated URL than before - in fact, using a URL from the Wikimedia `pageviews` system you dealt with through the pageviews package, which is stored as `url`. Without looking too much at the structure for the time being (we'll get to that later) this request asks for the number of pageviews to the English-language Wikipedia's "Hadley Wickham" article on 1 and 2 January 2017.

### Instructions

httr is loaded in your workspace.

- Make a GET request using the `url` object as the URL. Save the results as `pageview_response`.
- Call `content()` on `pageview_response` to retrieve the data the server has sent back. Save the data as `pageview_data`.
- Examine `pageview_data` with `str()`.

### script.R

```{r}
library(httr)

url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102"

# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```

## Multiple Choice: GET and POST requests {.tabset .tabset-fade .tabset-pills}

### Question

We've now discussed multiple types of HTTP request - including GET requests, for retrieving data, and POST requests, for transmitting it - as well as how to extract the server response once a request is complete. What sort of request would you use to send a server data?

### Answer

Possible Answers

[x] GET request      
[o] POST request

## Graceful httr

Error handling

- Every response includes a HTTP status code

    ```
    > response <- GET("https://httpbin.org/get")       
      Response [https://httpbin.org/get]      
      Date: 2017-08-24 20:29     
      Status: 200     
      Content-Type: application/json Size: 330 B      
      { ...    
    ```

Understanding status codes

- Code starts with: 
    - 2 - great!
    - 3 - great!
    - 4 - your code is broken 5 - their code is broken
- https://en.wikipedia.org/wiki/List_of_HTTP_status_codes
- Check for bad codes with http_error()

URL construction

- Most of URL doesn't change
- Stitch URLs together from bits that don't change with the bits that do 
- Saves thinking and typing

Directory-based URLs

- Slash-separated, like directories 
- https://fakeurl.com/api/peaches/thursday
- Use paste(), with sep = "/"

Parameter-based URLs

- Uses URL parameters (a=1&b=2) 
- https://fakeurl.com/api.php?fruit=peaches&day=thursday 
- Use GET() to construct the URL with query argument

## Handling http failures {.tabset .tabset-fade .tabset-pills}

### Exercies

As mentioned, HTTP calls can go wrong. Handling that can be done with httr's http_error() function, which identifies whether a server response contains an error.

If the response does contain an error, calling `http_error()` over the response will produce `TRUE`; otherwise, `FALSE`. You can use this for really fine-grained control over results. For example, you could check whether the request contained an error, and (if so) issue a warning and re-try the request.

For now we'll try something a bit simpler - issuing a warning that something went wrong if `http_error()` returns `TRUE`, and printing the content if it doesn't.

### Instructions

- Make a httr `GET()` request to the URL stored as `fake_url`, and store the result as `request_result`.
- If `http_error()` returns `TRUE`, use `warning()` to issue the warning "The request failed".
- If not, use `content()` (as demonstrated in previous exercises) to print the contents of the result.


### script.R

```{r, collapse=TRUE}
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	warning("The request failed")
} else {
	content(request_result)
}
```

## Constructing queries (Part I){.tabset .tabset-fade .tabset-pills}

### Exercies

As briefly discussed in the previous video, the actual API query (which tells the API what you want to do) tends to be in one of the two forms. The first is directory-based, where values are separated by `/` marks within the URL. The second is parameter-based, where all the values exist at the end of the URL and take the form of `key=value`.

Constructing directory-based URLs can be done via paste(), which takes an unlimited number of strings, along with a separator, as sep. So to construct `http://swapi.co/api/vehicles/12` we'd call:

> paste("http://swapi.co", "api", "vehicles", "12", sep = "/")

Let's do that now! We'll cover parameter-based URLs later. In the mean time we can play with SWAPI, mentioned above, which is an API chock full of star wars data. This time, rather than a vehicle, we'll look for a person.

### Instructions

httr is loaded in your workspace.

- Construct a directory-based API URL to `http://swapi.co/api`, looking for person `1` in `people`.
- Assign the URL to `directory_url`.
- Use `GET` to make an API call with `directory_url`.

### script.R

```{r, collapse=TRUE}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")

# Make a GET call with it
result <- GET(directory_url)
```

## Constructing queries (Part II){.tabset .tabset-fade .tabset-pills}

### Exercies

As mentioned (albeit briefly) in the last exercise, there are also parameter based URLs, where all the query values exist at the end of the URL and take the form of `key=value` - they look something like `http://fakeurl.com/foo.php?country=spain&food=goulash`

Constructing parameter-based URLs can also be done with `paste()`, but the easiest way to do it is with `GET()` and `POST()` themselves, which accept a `query` argument consisting of a list of keys and values. So, to continue with the food-based examples, we could construct `fakeurl.com/api.php?fruit=peaches&day=thursday` with:

> GET("fakeurl.com/api.php", query = list(fruit = "peaches", day = "thursday"))

In this exercise you'll construct a call to `https://httpbin.org/get?nationality=americans&country=antigua`

### Instructions

- Start by contructing the `query_params` list, with a `nationality` parameter of `"americans"` and a `country` parameter of `"antigua"`.
- Construct a parameter-based call to `https://httpbin.org/get`, using `GET()` passing `query_params` to the `query` arugment.
- Print the response `parameter_response`.

### script.R

```{r, collapse=TRUE}
# Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response
```

## Respectful API usage

User agents

- Bits of text that ID your browser (or software)
- Gives the server some idea of what you're trying to do 
- You can set one with your requests with user_agent() 
- Add an email address so they can contact you.

Rate limiting

- Too many requests makes for a sad server
- Deliberately slows down your code to keep under a desired 'rate' 
- Slows you, but avoids getting you banned from the server

## Using user agents{.tabset .tabset-fade .tabset-pills}

### Exercies

As discussed in the video, informative user-agents are a good way of being respectful of the developers running the API you're interacting with. They make it easy for them to contact you in the event something goes wrong. I always try to include:

1. My email address;
2. A URL for the project the code is a part of, if it's got a URL.

Building user agents is done by passing a call to `user_agent()` into the `GET()` or `POST()` request; something like:

`GET("http://url.goes.here/", user_agent("somefakeemail@domain.com http://project.website"))`

In the event you don't have a website, a short one-sentence description of what the project is about serves pretty well.

### Instructions

- Make a `GET()` request to `url`.
- Include a user agent that has a fake email address `"my@email.address"` followed by the sentence `"this is a test"`.
- Assign the response to `server_response`.

### script.R

```{r, collapse=TRUE}
# Do not change the url
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
```

## Rate-limiting {.tabset .tabset-fade .tabset-pills}

### Exercies
The next stage of respectful API usage is rate-limiting: making sure you only make a certain number of requests to the server in a given time period. Your limit will vary from server to server, but the implementation is always pretty much the same and involves a call to [`Sys.sleep()`](https://www.rdocumentation.org/packages/base/topics/Sys.sleep). This function takes one argument, a number, which represents the number of seconds to "sleep" (pause) the R session for. So if you call `Sys.sleep(15)`, it'll pause for 15 seconds before allowing further code to run.

As you can imagine, this is really useful for rate-limiting. If you are only allowed 4 requests a minute? No problem! Just pause for 15 seconds between each request and you're guaranteed to never exceed it. Let's demonstrate now by putting together a little loop that sends multiple requests on a 5-second time delay. We'll use `httpbin.org` 's APIs, which allow you to test different HTTP libraries.

### Instructions

- Construct a vector of 2 URLs, `http://httpbin.org/status/404` and `http://httpbin.org/status/301`.
- Write a for-loop that sends a `GET()` request to each one.
- Ensure that the for-loop uses `Sys.sleep()` to delay for 5 seconds between request.

### script.R

```{r, collapse=TRUE}
# Construct a vector of 2 URLs
urls <- c("http://httpbin.org/status/404", "http://httpbin.org/status/301")

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(5)
}
```

## Tying it all together{.tabset .tabset-fade .tabset-pills}

### Exercies

Tying it all together

Using everything that you learned in the chapter, let's make a simple replica of one of the 'pageviews' functions - building queries, sending GET requests (with an appropriate user agent) and handling the output in a fault-tolerant way. You'll build this function up piece by piece in this exercise.

To output an error, you will use the function stop(), which takes a string as an argument, stops the execution of the program, and outputs the string as an error. You can try it right now by running `stop("This is an error")`.

### Instructions

STEP1:
First, get the function to construct the url.

- In the call to paste(), add article_title as the second argument to construct url.

STEP2:
Now, make the request.

- Use GET() to request url with a user agent "my@email.com this is a test".

STEP3:
Now, add an error check.

- Check the response for errors with http_error(), throwing an error of "the request failed" with stop() if there was one.

STEP4:

- Finally, instead of returning response, return the content() of the response.

### script.R

```{r}
get_pageviews <- function(article_title){
  url <- paste(
    "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents", 
    # Include article title
    article_title, 
    "daily/2015100100/2015103100", 
    sep = "/"
  )   
  # Get the webpage  
  response <- GET(url, user_agent("my@email.com this is a test")) 
  # Is there an HTTP error?
  if(http_error(response)){ 
    # Throw an R error
    stop("the request failed") 
  }
  # Return the response's content
  content(response)
}
```



-------



# Handling JSON and XML

## JSON

JSON (JavaScript Object Notation)

- http://www.json.org/
- Plain text format Two structures:
- objects: {"title" : "A New Hope", "year" : "1977"}
- arrays: [1977, 1980]
- Values: "string", 3, true, false, null, or another object or array

An example JSON data set

```
[
  {
    "title": "A New Hope",
    "year" : 1977 },
  {
    "title" : "The Empire Strikes Back",
    "year" : 1980 
  }
]
```

Indentifying a JSON response

```
> library(httr)
> url <- "http://httpbin.org/get" > r <- GET(url)
> http_type(r)
[1] "application/json"
```

Indentifying a JSON response

```
> writeLines(content(r, as = "text"))
No encoding supplied: defaulting to UTF-8.
{
  "args": {},
  "headers": {
    "Accept": "application/json, text/xml, application/xml, */*", 
    "Accept-Encoding": "gzip, deflate",
    "Connection": "close",
    "Host": "httpbin.org",
    "User-Agent": "libcurl/7.54.0 r-curl/2.8.1 httr/1.2.1" 
  },
  "origin": "98.232.182.170",
  "url": "http://httpbin.org/get" 
}
```

## Can you spot JSON? {.tabset .tabset-fade .tabset-pills}

### Questions

Here is some information on a couple of fictional Jasons stored in different formats. Which one is JSON?

A:

```
<jason>
  <person>
    <first_name>Jason</first_name>
    <last_name>Bourne</last_name>
    <occupation>Spy</occupation>
  </person>
  <person>
    <first_name>Jason</first_name>
    <last_name>Voorhees</last_name>
    <occupation>Mass murderer</occupation>
  </person>
</jason>
```

B:

```
first_name, last_name, occupation
Jason, Bourne, Spy
Jason, Voorhees, Mass murderer
```

C:

```
[{ 
   "first_name": "Jason",
   "last_name": "Bourne",
   "occupation": "Spy"
 },
{
  "first_name": "Jason",
  "last_name": "Voorhees",
  "occupation": "Mass murderer"
}]
```

### Answer

Possible Answers

[x] A     
[x] B       
[o] C      

## Parsing JSON {.tabset .tabset-fade .tabset-pills}

### Exercise

While JSON is a useful format for sharing data, your first step will often be to parse it into an R object, so you can manipulate it with R.

The content() function in `httr` retrieves the content from a request. It takes an `as` argument that specifies the type of output to return. You've already seen that `as = "text"` will return the content as a character string which is useful for checking the content is as you expect.

If you don't specify `as`, the default `as = "parsed"` is used. In this case the type of `content()` will be guessed based on the header and `content()` will choose an appropriate parsing function. For JSON this function is fromJSON() from the jsonlite package. If you know your response is JSON, you may want to use `fromJSON()` directly.

To practice, you'll retrieve some revision history from the Wikipedia API, check it is JSON, then parse it into a list two ways.

### Instructions

- Get the revision history for the Wikipedia article for `"Hadley Wickham"`, by calling `rev_history("Hadley Wickham")` (a function we have written for you), store it in `resp_json`.
- Check the `http_type()` of `resp_json`, to confirm the API returned a JSON object.
- You can't always trust a header, so check the content looks like JSON by calling `content()` on `resp_json` with an additional argument, `as = "text"`.
- Parse `resp_json` using `content()` by explicitly setting `as = "parsed"`.
- Parse the returned text (from step 3) with `fromJSON()` .

### script.R

```{r, collapse=TRUE, eval=FALSE}

# url <- "https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=json&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0"
# resp_json <- GET(url)
# saveRDS(resp_json, "data/had_rev_json.rds")

rev_history <- function(title, format = "json"){
  if (title != "Hadley Wickham") {
    stop('rev_history() only works for `title = "Hadley Wickham"`')
  }
  if (format == "json"){
    resp <- readRDS("data//had_rev_json.rds")
  } else if (format == "xml"){
    resp <- readRDS("data/had_rev_xml.rds")
  } else {
    stop('Invalid format supplied, try "json" or "xml"')
  }
  resp  
}

# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")

# Check http_type() of resp_json
http_type(resp_json)

# Examine returned text with content()
content(resp_json, as = "text")

# Parse response with content()
content(resp_json, as = "parsed")

# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as = "text"))
```

## Manipulating JSON

Movies example

```{r, collapse=TRUE}
library(jsonlite)

# Create a simple json format example
movies_json <- '
[
  {
    "title" : "A New Hope", 
    "year" : 1977
  },
  {
    "title" : "The Empire Strikes Back",
    "year" : 1980
  }
]'

fromJSON(movies_json, simplifyVector = FALSE)

```

Simplifying the output (I)

simplifyVector = TRUE (arrays of primitives become vectors)

```{r, collapse=TRUE}
fromJSON(movies_json, simplifyVector = TRUE)
```

Simplifying the output (II)

simplifyDataFrame = TRUE (arrays of objects become data frames)

```{r, collapse=TRUE}
fromJSON(movies_json, simplifyDataFrame = TRUE)
```

Extracting data from JSON (I)

```{r, collapse=TRUE}
fromJSON(movies_json, simplifyDataFrame = TRUE)$title
```

Extracting data from JSON (II)

Iterate over list

- rlist
- base
- tidyverse

## Manipulating parsed JSON{.tabset .tabset-fade .tabset-pills}

### Exercise

As you saw in the video, the output from parsing JSON is a list. One way to extract relevant data from that list is to use a package specifically designed for manipulating lists, `rlist`.

`rlist` provides two particularly useful functions for selecting and combining elements from a list: list.select() and list.stack(). `list.select()` extracts sub-elements by name from each element in a list. For example using the parsed movies data from the video (`movies_list`), we might ask for the `title` and `year` elements from each element:

`list.select(movies_list, title, year)`

The result is still a list, that is where list.stack() comes in. It will stack the elements of a list into a data frame.

```
list.stack(
    list.select(movies_list, title, year)
)
```

In this exercise you'll use these `rlist` functions to create a data frame with the user and timestamp for each revision.


### Instrucitons

- First, you'll need to figure out where the revisions are. 
- Examine the output from the str() call. Can you see where the list of 5 revisions is?
- Store the revisions in revs.
- Use list.select() to pull out the user and timestamp elements from each revision, store in user_time.
- Print user_time to verify it's a list with one element for each revision.
- Use list.stack() to stack the lists into a data frame.

### script.R

```{r, include=FALSE}
resp_json <- readRDS("data/had_rev_json.rds")
```


```{r, collapse=TRUE}
# Load rlist
library(rlist)

# Examine output of this code
str(content(resp_json), max.level = 4)

# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract the user element
user_time <- list.select(revs, user, timestamp)

# Print user_time
user_time

# Stack to turn into a data frame
list.stack(user_time)
```

## Reformatting  {.tabset .tabset-fade .tabset-pills}

### Exercise

Of course you don't have to use `rlist`. You can achieve the same thing by using functions from base R or the tidyverse. In this exercise you'll repeat the task of extracting the username and timestamp using the `dplyr` package which is part of the tidyverse.

Conceptually, you'll take the list of revisions, stack them into a data frame, then pull out the relevant columns.

`dplyr`'s `bind_rows()` function takes a list and turns it into a data frame. Then you can use `select()` to extract the relevant columns. And of course if we can make use of the `%>%` (pipe) operator to chain them all together.

Try it!

### Instructions

- Pipe the list of revisions into `bind_rows()`.
- Use `select()` to extract the `user` and `timestamp` columns.

### script.R

```{r, collapse=TRUE}
# Load dplyr
library(dplyr)

# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract user and timestamp
revs %>%
  bind_rows %>%           
  select(user, timestamp)
```

## XML structure

Movies in XML

```
<?xml version="1.0" encoding="UTF-8"?> <movies>
<movie>
<title>A New Hope</title> <year>1977</year>
</movie> <movie>
<title>The Empire Strikes Back</title>
<year>1980</year> </movie>
</movies>
```

- Tags: <tagname>... </tagname>.
- E.g. <movies>, <movie>, <title>, <year>

Tags can have attributes

```
<?xml version="1.0" encoding="UTF-8"?> <movies>
<movie>
<title year = "1977">A New Hope</title>
</movie> <movie>
<title year = "1980">The Empire Strikes Back</title> </movie>
</movies>
```

The hierarchy of XML elements

```{r, out.width="50%"}
knitr::include_graphics("images/xml_1.png")
knitr::include_graphics("images/xml_2.png")
knitr::include_graphics("images/xml_3.png")
```

Understanding XML as a tree

```{r, out.width="50%"}
knitr::include_graphics("images/xml_4.png")
knitr::include_graphics("images/xml_5.png")
knitr::include_graphics("images/xml_6.png")
knitr::include_graphics("images/xml_7.png")
```

## Do you understand XML structure?


## Examining XML documents


## XPATHs


## Extracting XML data


## Extracting XML attributes

## Wrapup: returning nice API output





-------

# Web scraping with XPATHs